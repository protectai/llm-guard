site_name: LLM Guard
site_author: Protect AI, Inc.
site_description: LLM Guard is a comprehensive tool designed to fortify the security of Large Language Models (LLMs).
site_url: https://protectai.github.io/llm-guard
repo_url: https://github.com/protectai/llm-guard
repo_name: protectai/llm-guard
edit_uri: ""

copyright: |
  Maintained by <a href="https://protectai.com">Protect AI, Inc.</a>.

theme:
  name: material
  custom_dir: docs/overrides
  favicon: assets/favicon.ico
  logo: assets/logo.png
  icon:
    repo: fontawesome/brands/github
    admonition:
      note: octicons/tag-16
      abstract: octicons/checklist-16
      info: octicons/info-16
      tip: octicons/squirrel-16
      success: octicons/check-16
      question: octicons/question-16
      warning: octicons/alert-16
      failure: octicons/x-circle-16
      danger: octicons/zap-16
      bug: octicons/bug-16
      example: octicons/beaker-16
      quote: octicons/quote-16
  features:
    - announce.dismiss
    - navigation.instant
    - navigation.tracking
    - navigation.tabs
    - navigation.tabs.sticky
    - navigation.path
    - content.code.annotate
    - toc.follow
    - toc.integrate
    - navigation.top
    - content.code.copy
    - content.code.annotate
  palette:
    - scheme: default
      primary: custom
      toggle:
        icon: material/toggle-switch
        name: Switch to dark mode

    # Palette toggle for dark mode
    - scheme: slate
      primary: custom
      toggle:
        icon: material/toggle-switch-off-outline
        name: Switch to light mode


markdown_extensions:
  - pymdownx.highlight
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.tasklist
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - pymdownx.superfences
  - admonition
  - pymdownx.details
  - tables
  - def_list

plugins:
  - search
  - swagger-ui-tag:
      supportedSubmitMethods: []
      syntaxHighlightTheme: monokai
  - mkdocs-jupyter
  - git-revision-date-localized


extra:
  social:
    - icon: fontawesome/brands/twitter
      link: https://twitter.com/ProtectAICorp
    - icon: fontawesome/brands/github
      link: https://github.com/protectai
    - icon: fontawesome/brands/linkedin
      link: https://www.linkedin.com/company/protect-ai
    - icon: fontawesome/brands/slack
      link: https://mlsecops.com/slack
    - icon: fontawesome/solid/globe
      link: https://protectai.com/llm-guard
  analytics:
    provider: google
    property: G-B76J8KGG5Z

nav:
  - Get Started:
    - Index: index.md
    - Installation: get_started/installation.md
    - Quickstart: get_started/quickstart.md
    - Playground: get_started/playground.md
    - Attacks: get_started/attacks.md
    - Best Practices: get_started/best_practices.md
  - Tutorials:
    - Optimization: tutorials/optimization.md
    - Local Models: tutorials/notebooks/local_models.ipynb
    - OpenAI SDK: tutorials/openai.md
    - Across 100+ LLMs (LiteLLM): tutorials/litellm.md
    - Langchain: tutorials/notebooks/langchain.ipynb
    - Retrieval-augmented Generation: tutorials/rag.md
    - Secure RAG with Langchain: tutorials/notebooks/langchain_rag.ipynb
    - Secure RAG with LlamaIndex: tutorials/notebooks/llama_index_rag.ipynb
    - Secure Agents with Langchain: tutorials/notebooks/langchain_agents.ipynb
    - Invisible Prompt Attack: tutorials/attacks/invisible_prompt.ipynb
  - Input Scanners:
      - Anonymize: input_scanners/anonymize.md
      - BanCode: input_scanners/ban_code.md
      - Ban Competitors: input_scanners/ban_competitors.md
      - Ban Substrings: input_scanners/ban_substrings.md
      - Ban Topics: input_scanners/ban_topics.md
      - Code: input_scanners/code.md
      - Gibberish: input_scanners/gibberish.md
      - Invisible Text: input_scanners/invisible_text.md
      - Language: input_scanners/language.md
      - Prompt Injection: input_scanners/prompt_injection.md
      - Regex: input_scanners/regex.md
      - Secrets: input_scanners/secrets.md
      - Sentiment: input_scanners/sentiment.md
      - Token Limit: input_scanners/token_limit.md
      - Toxicity: input_scanners/toxicity.md
  - Output Scanners:
      - Ban Competitors: output_scanners/ban_competitors.md
      - Ban Substrings: output_scanners/ban_substrings.md
      - Ban Topics: output_scanners/ban_topics.md
      - Bias: output_scanners/bias.md
      - Code: output_scanners/code.md
      - Deanonymize: output_scanners/deanonymize.md
      - JSON: output_scanners/json.md
      - Language: output_scanners/language.md
      - Language Same: output_scanners/language_same.md
      - Malicious URLs: output_scanners/malicious_urls.md
      - No Refusal: output_scanners/no_refusal.md
      - Reading Time: output_scanners/reading_time.md
      - Factual Consistency: output_scanners/factual_consistency.md
      - Gibberish: output_scanners/gibberish.md
      - Regex: output_scanners/regex.md
      - Relevance: output_scanners/relevance.md
      - Sensitive: output_scanners/sensitive.md
      - Sentiment: output_scanners/sentiment.md
      - Toxicity: output_scanners/toxicity.md
      - URL Reachability: output_scanners/url_reachability.md
  - API:
    - Overview: api/overview.md
    - Deployment: api/deployment.md
    - API Reference: api/reference.md
    - Client: api/client.md
  - Changelog: changelog.md
  - Customization:
      - Add scanner: customization/add_scanner.md

extra_css:
  - assets/extra.css
